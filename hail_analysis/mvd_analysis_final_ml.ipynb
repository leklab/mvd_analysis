{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "from pcgc_hail.hail_scripts.utils import *\n",
    "from pcgc_hail.hail_scripts.annotate_frequencies import *\n",
    "\n",
    "# The master will need to be edited on each run\n",
    "hl.init(master='spark://c18n05.ruddle.hpc.yale.internal:7077')\n",
    "\n",
    "# Starting with the VEP annotated matrix table\n",
    "mt = hl.read_matrix_table('/gpfs/ycga/scratch60/ysm/lek/njl27/gruber_exomes/spark_on_slurm/hail_output/gruber_WES_b5_vep.mt')\n",
    "\n",
    "# sort transcript consequences \n",
    "mt = mt.annotate_rows(\n",
    "    sortedTranscriptConsequences=get_expr_for_vep_sorted_transcript_consequences_array(vep_root=mt.vep)\n",
    ")\n",
    "\n",
    "# annotate rows with index zero element of array sorted transcript annotations\n",
    "# hl.null was replaced with hl.missing as the former has been deprecated\n",
    "# The code below is ugly and the checks are redundant and possibly overkill\n",
    "mt = mt.annotate_rows(\n",
    "    gene_symbol=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].gene_symbol, hl.missing(hl.tstr)), \n",
    "    ENSG=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].gene_id, hl.missing(hl.tstr)), \n",
    "    ENST=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].transcript_id, hl.missing(hl.tstr)), \n",
    "    major_consequence=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].major_consequence, hl.missing(hl.tstr)), \n",
    "    hgvsc=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].hgvsc, hl.missing(hl.tstr)), \n",
    "    hgvs=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].hgvs, hl.missing(hl.tstr)), \n",
    "    category=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].category, hl.missing(hl.tstr)), \n",
    "    canonical=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].canonical, -1),\n",
    "    polyphen_pred=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].polyphen_prediction, hl.missing(hl.tstr)),\n",
    "    sift_pred=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].sift_prediction, hl.missing(hl.tstr))\n",
    ")\n",
    "\n",
    "# annotate rows with additional fields from VEP parent structure before dropping\n",
    "mt = mt.annotate_rows(\n",
    "    most_severe_consequence = mt.vep.most_severe_consequence,\n",
    "    variant_class = mt.vep.variant_class,\n",
    "    gene_id_array = mt.vep.transcript_consequences.gene_id,\n",
    "    gene_symbol_array = mt.vep.transcript_consequences.gene_symbol \n",
    ")\n",
    "\n",
    "# drop parent vep structure \n",
    "mt = mt.drop(mt.vep)\n",
    "\n",
    "# VEP filtering is done first as it results in the largest reduction in rows. The call set at the moment is currently\n",
    "# dominated by variants that are intronic or intergenic.\n",
    "\n",
    "# Previously the code below had a bunch of string comparison and \"or\" separating them.\n",
    "# This is pretty ugly code to do it this way. It's better to have a set and check for membership (using contains).\n",
    "deleterious_consequences = hl.set(['splice_acceptor_variant', 'splice_donor_variant', 'stop_gained',\n",
    "                                  'stop_lost','missense_variant', 'frameshift_variant', 'start_lost',\n",
    "                                  'inframe_insertion', 'inframe_deletion', 'protein_altering_variant'])\n",
    "\n",
    "#((mt.major_consequence == \"missense_variant\")&((mt.sift_prediction == \"deleterious\") | (mt.sift_prediction == \"deleterious_low_confidence\") | (mt.polyphen_prediction == \"probably_damaging\") | (mt.polyphen_prediction == \"possibly_damaging\"))) |\n",
    "#(mt.major_consequence == \"splice_region_variant\"), \n",
    "\n",
    "# Try to use mt.count() sparingly as it needs to pass over the entire matrix table.\n",
    "print('Count of the input dataframe before filtering:' + str(mt.count()) + '\\n')\n",
    "\n",
    "# major VEP consequence filtering \n",
    "mt = mt.filter_rows(deleterious_consequences.contains(mt.major_consequence),keep = True)\n",
    "print('Count of the dataframe after also filtering by VEP consequence' + str(mt.count()) + '\\n')\n",
    "\n",
    "\n",
    "# add variant_qc - https://hail.is/docs/0.2/methods/genetics.html#hail.methods.variant_qc\n",
    "mt = hl.variant_qc(mt, name='variant_qc')\n",
    "\n",
    "\n",
    "# Add in gnomADv2 allele frequencies. This file has annotation for every site in the gemome.\n",
    "\n",
    "# This hail table is approximately ~300 Gb so passing this file around to the SPARK workers is extremely slow.\n",
    "# Two ways of speeding this up: 1. subset this hail table down to just generic regions so the hail table will only\n",
    "# be around 3-10 Gb. 2. Repartition the matrix table we want to annotate. For this project it's currently at\n",
    "# 30 partitions, thus limiting it to only 30 concurrent jobs that can be run.\n",
    "\n",
    "combined_ref = hl.read_table('/gpfs/gibbs/pi/brueckner/hail_resources/combined_reference_data_grch38.ht')\n",
    "\n",
    "mt = mt.annotate_rows(\n",
    "    gnomad_genomes_af = combined_ref[mt.row_key].gnomad_genomes.AF,\n",
    "    gnomad_exomes_af = combined_ref[mt.row_key].gnomad_exomes.AF,\n",
    "    primate_ai = combined_ref[mt.row_key].primate_ai.score,\n",
    "    splice_ai = combined_ref[mt.row_key].splice_ai.delta_score,\n",
    "    meta_svm = combined_ref[mt.row_key].dbnsfp.MetaSVM_pred,\n",
    "    #dbnsfp_polyphen2 = combined_ref[mt.row_key].dbnsfp.Polyphen2_HVAR_pred,\n",
    "    #dbnsfp_sift = combined_ref[mt.row_key].dbnsfp.SIFT_pred,\n",
    "    cadd = combined_ref[mt.row_key].cadd.PHRED    \n",
    ")\n",
    "\n",
    "mt = mt.annotate_rows(\n",
    "    gnomad_af = hl.if_else(hl.is_defined(mt.gnomad_exomes_af) | hl.is_defined(mt.gnomad_genomes_af), \n",
    "                           hl.if_else(hl.is_defined(mt.gnomad_exomes_af), mt.gnomad_exomes_af, mt.gnomad_genomes_af), \n",
    "                           0.0)\n",
    "    )\n",
    "\n",
    "\n",
    "# write out filtered matrix table. Note when writing out using Jupyter notebook, need to use the full path otherwise\n",
    "# it will happily write out without error but the directories are empty!\n",
    "print('Writes out filtered to the following matrix table')\n",
    "mt.write('/gpfs/ycga/scratch60/ysm/lek/ml2529/gruber_analysis/gruber_WES_b5_vep_conseq_filtered.mt', overwrite = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab05b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code block that does the non-VEP filtering\n",
    "\n",
    "mt = hl.read_matrix_table('/gpfs/ycga/scratch60/ysm/lek/ml2529/gruber_analysis/gruber_WES_b5_vep_conseq_filtered.mt')\n",
    "\n",
    "# filter by AC_adj, which excludes variant calls below quality thresholds\n",
    "# these are: genotype quality (GQ) >= 20, depth (DP) >=10, and allele balance (AB) >= 0.2 and <= 0.8 (for heterozygous genotypes only)\n",
    "mt = annotate_adj(mt)\n",
    "mt = mt.annotate_rows(AC_adj = hl.agg.filter(mt.adj, hl.agg.call_stats(mt.GT, mt.alleles).AC[1]))\n",
    "\n",
    "print('Count of the dataframe BEFORE quality and gnomAD AF filtering:' + str(mt.count()) + '\\n')\n",
    "\n",
    "# filter by PASS status, per https://hail.is/docs/0.2/methods/impex.html\n",
    "mt = mt.filter_rows((hl.len(mt.filters) == 0) & \n",
    "                   (mt.AC_adj > 0) &\n",
    "                   (mt.gnomad_af <= 0.01), keep=True)\n",
    "\n",
    "print('Count of the dataframe AFTER quality and gnomAD AF filtering:' + str(mt.count()) + '\\n')\n",
    "\n",
    "# write out filtered matrix table\n",
    "print('Writes out filtered to the following matrix table')\n",
    "mt.write('/gpfs/ycga/scratch60/ysm/lek/ml2529/gruber_analysis/gruber_WES_b5_vep_filtered_FINAL.mt', overwrite = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4981c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate with sample information and write out to file\n",
    "# read in filtered matrix table\n",
    "#mt_output = hl_outdir + 'gruber_WES_b5_vep_plus_filtered.mt' \n",
    "\n",
    "import hail as hl\n",
    "mt = hl.read_matrix_table('/gpfs/ycga/scratch60/ysm/lek/ml2529/gruber_analysis/gruber_WES_b5_vep_filtered_FINAL.mt')\n",
    "\n",
    "# sample information \n",
    "file_input = 'sample_groups.txt'\n",
    "print('Reads in the following txt as a hail table: \\n' + file_input + '\\n')\n",
    "ht = hl.import_table(file_input).key_by('Adult-ID')\n",
    "ht = ht.rename({'Adult-ID' : 's'}) #to match mt\n",
    "\n",
    "# add phenoype group as column annotation\n",
    "mt = mt.annotate_entries(phenotype = ht[mt.s].group)\n",
    "mt.describe()\n",
    "\n",
    "# annotate with AC_adj for each phenotype group\n",
    "mt = mt.annotate_rows(AC_adj_Barlow = hl.agg.filter((mt.phenotype == \"Barlow\") & mt.adj, hl.agg.call_stats(mt.GT, mt.alleles).AC[1]))\n",
    "mt = mt.annotate_rows(AC_adj_FED = hl.agg.filter((mt.phenotype == \"FED\") & mt.adj, hl.agg.call_stats(mt.GT, mt.alleles).AC[1]))\n",
    "mt.rows().show(5)\n",
    "\n",
    "# annotate if in gene list\n",
    "ht = hl.import_table('gruber_gene_list.txt',delimiter='\\t').key_by('Gene_Symbol')\n",
    "ht = ht.rename({'Gene_Symbol' : 'gene_symbol'}) #to match mt\n",
    "ht.show()\n",
    "mt = mt.annotate_rows(\n",
    "    in_gene_list = hl.if_else(hl.is_defined(ht[mt.gene_symbol].gene_annotation), 'YES', 'NO'),\n",
    "    gene_annotation = hl.if_else(hl.is_defined(ht[mt.gene_symbol].gene_annotation), ht[mt.gene_symbol].gene_annotation, 'NA')\n",
    ")\n",
    "mt.describe()\n",
    "\n",
    "\n",
    "# Frequently mutated genes. https://bmcmedgenomics.biomedcentral.com/articles/10.1186/s12920-014-0064-y\n",
    "ht = hl.import_table('flag_gene_list.txt',delimiter='\\t').key_by('gene_symbol')\n",
    "ht.show()\n",
    "mt = mt.annotate_rows(\n",
    "    flag_gene_list = hl.if_else(hl.is_defined(ht[mt.gene_symbol]), 'YES', 'NO'),\n",
    ")\n",
    "mt.describe()\n",
    "\n",
    "\n",
    "# use make_table to make a table from a matrix table with one field per sample.\n",
    "# perhaps this could be used to get variants calls per sample, subset for those with GT 0/1 or 1/1\n",
    "#mt = mt.make_table()\n",
    "\n",
    "# save as tsv\n",
    "#final_output = hl_outdir + 'mvd_WES_analysis.tsv' \n",
    "table = mt.rows()\n",
    "table.show()\n",
    "table = table.drop('filters','allele_data','a_index','was_split','vep_proc_id','sortedTranscriptConsequences','gene_id_array','gene_symbol_array')\n",
    "table.show()\n",
    "#table.write('/gpfs/ycga/scratch60/ysm/lek/ml2529/gruber_analysis/mvd_WES_analysis.ht')\n",
    "table.export('/gpfs/ycga/scratch60/ysm/lek/ml2529/gruber_analysis/mvd_WES_analysis_v3.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24668a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
